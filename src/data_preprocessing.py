# -*- coding: utf-8 -*-
"""Data_preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AV-C-m7sAaiId1fTufEm6TceO1-YW5OO

# Credit Scoring Model Development at Bati Bank
### This project focuses on developing a machine learning model to classify credit scores based on a person's credit-related information.
### Objectives of the Project
#### •	Develop a proxy variable to categorize users as high or low risk.
#### •	Select features with high correlation to the default variable.
#### •	Build a model that estimates risk probability for new customers.
#### •	Create a credit scoring system using risk probability estimates.
#### •	Optimize loan amount and duration using model predictions.
"""

!pip install skimpy

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from skimpy import clean_columns

import warnings
warnings.filterwarnings("ignore")
warnings.warn("this will not show")

plt.rcParams["figure.figsize"] = (10,6)

sns.set_style("whitegrid")
pd.set_option('display.float_format', lambda x: '%.3f' % x)

# Set it None to display all rows in the dataframe
# pd.set_option('display.max_rows', None)

# Set it to None to display all columns in the dataframe
pd.set_option('display.max_columns', None)

"""Read Data Set"""

df = pd.read_csv("/content/data.csv")

df.head()

print(df.info())

"""## This dataset provides information about various transactions:
   ### Column Breakdown:
             TransactionId: Unique identifier for each transaction.
             BatchId: Identifier for batches of transactions.
             AccountId: The account associated with the transaction.
             SubscriptionId: Identifier for subscription services linked to the transaction.
             CustomerId: Unique identifier for the customer.
             CurrencyCode: Currency used in the transaction (e.g., UGX for Ugandan Shilling).
             CountryCode: Numeric country code (e.g., 256).
             ProviderId: Identifier for the service provider (e.g., telecom, utility companies).
             ProductId: Identifier for the product sold (e.g., airtime, utility bill, financial services).
             ProductCategory: The category the product belongs to (e.g., airtime, financial_services, utility_bill).
             ChannelId: Identifier for the transaction channel (e.g., mobile, online).
             Amount: The amount transacted, usually in float format.
             Value: Corresponding value of the transaction (could be the total cost, including fees).
             TransactionStartTime: Timestamp of when the transaction started.
             PricingStrategy: Identifier for different pricing strategies applied to the transaction.
             FraudResult: Indicator for whether the transaction was flagged as fraudulent (0 for non-fraud, 1 for fraud).
   #### Considerations:
##### The Amount column includes positive and negative values (e.g., -20.000), which likely represent refunds, charges, or reversals.
#####  convert the TransactionStartTime to a proper datetime format for time-based analysis.
##### The FraudResult can be used for classification tasks, making this a supervised learning dataset if you aim to predict fraudulent transactions.
#### Next Steps:
   #####  Data Cleaning
   #####  Feature Engineering: Create new features, such as time-based features (hour, day, month) or aggregating transaction amounts by CustomerId.
   ##### Fraud Detection: we can build a model to classify fraudulent transactions using the FraudResult as the target.

**Data Cleaning**
"""

df.shape

df.describe().T

df.describe(include = "object").T

"""#### Summary Statistics:
CountryCode:

        Count: 95,662 entries.
        Mean: 256.
        Standard Deviation (std): 0.
        Min, 25%, 50%, 75%, Max: All values are 256.
        This suggests that all transactions are from the same country (likely Uganda, based on the country code 256).
Amount:

        Mean: 6,717.85.
        Std: 123,306.80, indicating high variability in transaction amounts.
        Min: -1,000,000, showing possible large refunds or reversals.
        25th percentile: -50, implying some transactions were negative.
        Median (50%): 1,000.
        75th percentile: 2,800.
        Max: 9,880,000, a very high transaction, which may indicate outliers.
Value:

        Mean: 9,900.58.
        Std: 123,122.09, again showing high variability.
        Min: 2.
        25th percentile: 275.
        Median: 1,000.
        75th percentile: 5,000.
        Max: 9,880,000, similar to the Amount column, indicating they are related or correlated.
PricingStrategy:

        Mean: 2.26, with a Std of 0.733.
        The values range from 0 to 4, indicating different pricing strategies applied to the transactions. The majority fall around 2.
FraudResult:

        Mean: 0.002, indicating a very low incidence of fraud.
        Std: 0.045.
        The min and 25th to 75th percentiles are all 0, with the max at 1.
        This suggests that fraud is rare, with only a small fraction of transactions marked as fraudulent (fraud rate is ~0.2%).
Observations:

        The dataset is dominated by non-fraudulent transactions, as seen in the FraudResult column.
        The Amount and Value columns have a high standard deviation and extreme values, which may indicate the presence of outliers or highly varied transaction sizes.
        CountryCode shows no variation, so it likely doesn't need further analysis unless multi-country data is expected.
        There are negative amounts, which should be investigated to understand if they represent refunds, reversals, or errors.
Next Steps:
    
    Outlier Detection: Since Amount and Value have extreme values, investigate outliers further, perhaps using a boxplot or z-scores.
    Fraud Analysis: Given the low fraud rate, you could try techniques like SMOTE for class imbalance before building fraud detection models.
Correlation:
        
        Check for correlations between Amount, Value, and FraudResult to see if higher amounts are associated with fraud.

**Missing Values**
"""

# Checking for missing values
missing_values = df.isnull().sum()
print(missing_values)

"""**Handling Negative Values in Amount**"""

# Check for negative values in 'Amount'
negative_amounts = df[df['Amount'] < 0]
print(f"Number of negative amounts: {len(negative_amounts)}")

# Display some examples of negative amounts
negative_amounts.head()

"""#### It looks like the dataset contains a significant number of transactions with negative values in the Amount column (38,189 occurrences).
These could represent refunds, charges, or errors depending on the business context. Here's a possible approach to handle them:
Investigating Negative Values
    If negative amounts correspond to valid transactions (e.g., refunds, fee reversals), you may want to keep them.
    If they are errors or irrelevant, you may decide to either remove or correct them.
"""

# Investigating the distribution of negative amounts by product category and channel
negative_distribution = df[df['Amount'] < 0].groupby(['ProductCategory', 'ChannelId'])['Amount'].count()
print(negative_distribution)

"""From the breakdown, it looks like most of the negative values are in the financial_services category under ChannelId_2 (32,629 occurrences), followed by airtime under both ChannelId_2 and ChannelId_5.

Insights from Breakdown:

    financial_services transactions have the most negative amounts. These could represent fees, refunds, or reversals that are typical for financial services.
    airtime also has significant negative transactions, possibly indicating refunds for airtime purchases.
    Other categories like utility_bill, data_bundles, and tv have much fewer negative amounts.

Approach Options:

    1. Keep Negative Transactions for Certain Categories
    If negative values in financial_services and airtime make sense in the context of refunds or reversals, let retain them. We can continue analysis with both positive and negative amounts.

    2. Remove or Adjust Only in Specific Categories
   
    For product categories where negative values are unexpected (like movies, tv, or utility_bill), you may decide to remove them or adjust them.

For instance, you could keep negative transactions for financial_services and airtime but remove them for the other categories.
"""

# Remove negative amounts for specific product categories
categories_to_remove = ['movies', 'tv', 'utility_bill', 'data_bundles']
df_cleaned = df[~((df['Amount'] < 0) & (df['ProductCategory'].isin(categories_to_remove)))]

# Keep or Adjust the Rest
df_cleaned.loc[df_cleaned['ProductCategory'].isin(['financial_services', 'airtime']), 'Amount'] = df_cleaned['Amount'].abs()

df.duplicated().sum()

"""### Visualize Distribution of Transaction Amounts"""

# We'll plot the distribution of the Amount field to visually inspect the outliers.
import seaborn as sns
import matplotlib.pyplot as plt

# Plotting the distribution of Amounts
plt.figure(figsize=(10, 6))
sns.boxplot(df_cleaned['Amount'])
plt.title('Boxplot of Transaction Amounts')
plt.show()

"""#### The boxplot  shows transaction amounts, where most of the data is concentrated at lower values, while there are some significant outliers (points far above the box)."""

# Apply log transformation (be cautious with negative values)
df['log_amount'] = np.log1p(df['Amount'])

# Cap extreme values at a certain threshold
cap_value = df['Amount'].quantile(0.99)
df['Amount_capped'] = np.where(df['Amount'] > cap_value, cap_value, df['Amount'])

#  Adjust IQR thresholds for stricter outlier removal
Q1 = df['Amount'].quantile(0.25)
Q3 = df['Amount'].quantile(0.75)
IQR = Q3 - Q1
df_cleaned = df[~((df['Amount'] < (Q1 - 1.5 * IQR)) | (df['Amount'] > (Q3 + 1.5 * IQR)))]

# We'll plot the distribution of the Amount field to visually inspect the outliers.
import seaborn as sns
import matplotlib.pyplot as plt

# Plotting the distribution of Amounts
plt.figure(figsize=(10, 6))
sns.boxplot(df_cleaned['Amount'])
plt.title('Boxplot of Transaction Amounts')
plt.show()

"""#### Data is now much cleaner after the outlier removal, with the boxplot showing a more reasonable distribution. The remaining outliers may represent anomalies or natural variation in your dataset."""

import numpy as np

# Apply log transformation (handling any non-positive values carefully)
df['Log_Amount'] = np.log1p(df['Amount'])  # log1p adds 1 to avoid issues with zero values

# Cap both upper and lower extremes
lower_cap = df['Amount'].quantile(0.01)
upper_cap = df['Amount'].quantile(0.99)

df['Amount_capped'] = df['Amount'].clip(lower_cap, upper_cap)

# We'll plot the distribution of the Amount field to visually inspect the outliers.
import seaborn as sns
import matplotlib.pyplot as plt

# Plotting the distribution of Amounts
plt.figure(figsize=(10, 6))
sns.boxplot(df_cleaned['Amount'])
plt.title('Boxplot of Transaction Amounts')
plt.show()

"""## Feature Engineering:"""

# Extract new features from the TransactionStartTime column
df['TransactionStartTime'] = pd.to_datetime(df['TransactionStartTime'])
df['Transaction_Hour'] = df['TransactionStartTime'].dt.hour
df['Transaction_Day'] = df['TransactionStartTime'].dt.day
df['Transaction_Weekday'] = df['TransactionStartTime'].dt.weekday
df['Transaction_Month'] = df['TransactionStartTime'].dt.month

# Encode categorical columns such as ProductCategory, ChannelId, ProviderId, etc., using methods like one-hot encoding or label encoding.
df = pd.get_dummies(df, columns=['ProductCategory', 'ChannelId', 'ProviderId'])

# Create ratios between the Amount and Value columns, which may indicate if a transaction is unusually high or low.
df['Amount_to_Value_Ratio'] = df['Amount'] / df['Value']

df['Amount_to_Value_Ratio'].head()

"""# Correlation Analysis:
#### Check for correlations between numerical variables (e.g., Amount, Value, PricingStrategy) and the FraudResult column to explore relationships and potential predictive power.
"""

numeric_df = df.select_dtypes(include=['float64', 'int64'])
corr_matrix = numeric_df.corr()

import seaborn as sns
import matplotlib.pyplot as plt

sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.show()

"""### Key Observations
High Correlations:

    Amount and Value: 0.99 (Very strong positive correlation)
    log_amount and Amount_capped: 0.8 (Strong positive correlation)
    Log_Amount and Amount_capped: 0.8 (Strong positive correlation)
Moderate Correlations:

    PricingStrategy and Amount: -0.062 (Moderate negative correlation)
    PricingStrategy and Value: -0.017 (Moderate negative correlation)
    FraudResult and Amount: 0.25 (Moderate positive correlation)
    FraudResult and Value: 0.27 (Moderate positive correlation)
Negative Correlations:

    PricingStrategy and Fraud Result: -0.26 (Moderate negative correlation)

1. Feature Selection:
    Based on high correlation between Amount and Value, it’s recommended to drop one of these to avoid redundancy. Since the correlation is nearly 1, there will be little loss of information by excluding one.
    For features like PricingStrategy that show moderate correlation with FraudResult, keep them in the model as they might help in fraud detection.
2. Handle Multicollinearity:
    Strong correlations between Log_Amount, Amount, and Amount_capped suggest multicollinearity. For model building, you should keep only one   version of the transformed Amount variable. If Amount_capped helps reduce outliers, it could be more effective to keep.
"""

# Dropping Value due to high correlation with Amount
df_cleaned = df.drop(columns=['Value'])

# Keeping Amount_capped and dropping other variations
df_cleaned = df_cleaned.drop(columns=['Amount', 'Log_Amount'])  # Drop original Amount and Log_Amount

# Displaying the cleaned DataFrame
print("Cleaned DataFrame:")
print(df_cleaned.head())

# Save the cleaned DataFrame to a CSV file
df_encoded.to_csv('/content/cleaned_data.csv', index=False)